{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8d9c3a7",
   "metadata": {},
   "source": [
    "## Lab Overview\n",
    "- Participants will learn how to fetch JSON data from a RESTful API (https://jsonplaceholder.typicode.com/posts/), convert it into a PySpark DataFrame, analyze its schema and content, and then efficiently save it into a MySQL database table. Through this lab, participants will gain practical skills in data ingestion, transformation, and persistence using Python, Spark, and MySQL, essential for building scalable data pipelines in real-world scenarios.\n",
    "### Learning Objective\n",
    "- Describe  how to retrieve JSON data from an API endpoint using Python and integrate it into a PySpark DataFrame for further processing.\n",
    "- Familiarize themselves with essential concepts of PySpark, including SparkSession initialization, DataFrame creation from JSON data, and schema inspection.\n",
    "- Explore Data Persistence Options: Explore methods for persisting PySpark DataFrames into a MySQL database, including defining connection properties, specifying JDBC URLs, and selecting appropriate write modes.\n",
    "- Perform hands-on experience in building a simple, effective data pipeline, from data retrieval to persistence, using popular Python libraries like requests, PySpark, and MySQL connector.\n",
    "- Build Foundation for Scalable Data Pipelines: Lay the foundation for understanding and building scalable data pipelines by integrating PySpark with external data sources and databases, essential for real-world data engineering tasks.\n",
    "### Dataset:\n",
    "- The “classicmodels” database.\n",
    "- API Endpoint URL = \"https://jsonplaceholder.typicode.com/posts/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b99037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b00ef49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- body: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- userId: long (nullable = true)\n",
      "\n",
      "+--------------------+---+--------------------+------+\n",
      "|                body| id|               title|userId|\n",
      "+--------------------+---+--------------------+------+\n",
      "|quia et suscipit\\...|  1|sunt aut facere r...|     1|\n",
      "|est rerum tempore...|  2|        qui est esse|     1|\n",
      "|et iusto sed quo ...|  3|ea molestias quas...|     1|\n",
      "|ullam et saepe re...|  4|eum et est occaecati|     1|\n",
      "|repudiandae venia...|  5|  nesciunt quas odio|     1|\n",
      "|ut aspernatur cor...|  6|dolorem eum magni...|     1|\n",
      "|dolore placeat qu...|  7|magnam facilis autem|     1|\n",
      "|dignissimos aperi...|  8|dolorem dolore es...|     1|\n",
      "|consectetur animi...|  9|nesciunt iure omn...|     1|\n",
      "|quo et expedita m...| 10|optio molestias i...|     1|\n",
      "|delectus reiciend...| 11|et ea vero quia l...|     2|\n",
      "|itaque id aut mag...| 12|in quibusdam temp...|     2|\n",
      "|aut dicta possimu...| 13|dolorum ut in vol...|     2|\n",
      "|fuga et accusamus...| 14|voluptatem eligen...|     2|\n",
      "|reprehenderit quo...| 15|eveniet quod temp...|     2|\n",
      "|suscipit nam nisi...| 16|sint suscipit per...|     2|\n",
      "|eos voluptas et a...| 17|fugit voluptas se...|     2|\n",
      "|eveniet quo quis\\...| 18|voluptate et itaq...|     2|\n",
      "|illum quis cupidi...| 19|adipisci placeat ...|     2|\n",
      "|qui consequuntur ...| 20|doloribus ad prov...|     2|\n",
      "+--------------------+---+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"Read and Save JSON Data with PySpark SQL\").getOrCreate()\n",
    "\n",
    "# API Endpoint URL\n",
    "api_url = \"https://jsonplaceholder.typicode.com/posts/\"\n",
    "\n",
    "# Fetch data from API\n",
    "response = requests.get(api_url)\n",
    "if response.status_code == 200:\n",
    "\n",
    "    # Convert JSON response to DataFrame \n",
    "    json_df = spark.read.json(spark.sparkContext.parallelize([response.json()]))\n",
    "\n",
    "    # Show DataFrame schema and content\n",
    "    json_df.printSchema()\n",
    "    json_df.show()\n",
    "\n",
    "    # Define MySQL connection properties\n",
    "    mysql_props = {\n",
    "        \"user\": \"root\",\n",
    "        \"password\": \"password\",\n",
    "        \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "    }\n",
    "    # JDBC URL for MySQL\n",
    "    mysql_url = \"jdbc:mysql://localhost:3306/usersdb\"\n",
    "    # Save DataFrame to MySQL table\n",
    "    json_df.write.jdbc(url=mysql_url, table=\"json_data_table\", mode=\"overwrite\", properties=mysql_props)\n",
    "else:\n",
    "    print(f\"Failed to fetch data from API. Status code: {response.status_code}\")\n",
    "# Stop SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fd8859",
   "metadata": {},
   "source": [
    "### Explanation of Process\n",
    "\n",
    "- Setting up PySpark:\n",
    "    - It uses findspark to initialize the PySpark environment.\n",
    "    - Imports pyspark and SparkSession from pyspark.sql.\n",
    "- Initializing SparkSession:\n",
    "    - Initializes a SparkSession named \"Read and Save JSON Data with PySpark SQL.\"\n",
    "- Fetching Data from API:\n",
    "    - Makes an HTTP GET request to the specified API endpoint (https://jsonplaceholder.typicode.com/posts/) using the requests.get() method.\n",
    "    - If the request is successful (status code 200), it converts the JSON response into a PySpark DataFrame using spark.read.json().\n",
    "- Data Processing:\n",
    "    - The DataFrame schema and content are printed using `json_df.printSchema()` and `json_df.show()` respectively.\n",
    "- Saving Data to MySQL:\n",
    "    - It defines MySQL connection properties (mysql_props) including username, password, and JDBC driver.\n",
    "    - Specifies the JDBC URL for connecting to the MySQL database.\n",
    "    - Saves the DataFrame into a MySQL table named \"json_data_table\" using the `write.jdbc()` method, with the mode set to \"overwrite\" to replace existing data if any.\n",
    "- Error Handling:\n",
    "    - Checks the HTTP response status code and prints an error message if the request fails.\n",
    "- Environment Cleanup:\n",
    "    - Stop the SparkSession to release resources.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
