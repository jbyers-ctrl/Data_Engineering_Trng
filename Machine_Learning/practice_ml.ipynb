{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4a8d2da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FindSpark simplifies the process of using Apache Spark with Python\n",
    "import requests\n",
    "import pyspark\n",
    "import findspark\n",
    "\n",
    "# Initializing FindSpark to locate Spark installation\n",
    "findspark.init()\n",
    "\n",
    "# Importing SparkSession from pyspark.sql module\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#import functions/Classes for sparkml\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# import functions/Classes for pipeline creation\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# import functions/Classes for metrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator, BinaryClassificationEvaluator, ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12bce2c",
   "metadata": {},
   "source": [
    "## Task 1 - Create a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21caa95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 52389)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\james.byers\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"C:\\Users\\james.byers\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"C:\\Users\\james.byers\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"C:\\Users\\james.byers\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"c:\\Users\\james.byers\\Data_Engineer\\.data\\lib\\site-packages\\pyspark\\accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"c:\\Users\\james.byers\\Data_Engineer\\.data\\lib\\site-packages\\pyspark\\accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"c:\\Users\\james.byers\\Data_Engineer\\.data\\lib\\site-packages\\pyspark\\accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"c:\\Users\\james.byers\\Data_Engineer\\.data\\lib\\site-packages\\pyspark\\serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "  File \"C:\\Users\\james.byers\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Classification using SparkML\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8561e9e5",
   "metadata": {},
   "source": [
    "## Task 2 - Load data from a CSV file into a dataframe\n",
    "Download data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9972ffd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+--------------+----------+------------+------+------+-------+-------------+-------------+\n",
      "|Application_ID|Application_Status|Credit_History|Dependents|   Education|Gender|Income|Married|Property_Area|Self_Employed|\n",
      "+--------------+------------------+--------------+----------+------------+------+------+-------+-------------+-------------+\n",
      "|      LP001002|                 Y|             1|         0|    Graduate|  Male|medium|     No|        Urban|           No|\n",
      "|      LP001003|                 N|             1|         1|    Graduate|  Male|medium|    Yes|        Rural|           No|\n",
      "|      LP001005|                 Y|             1|         0|    Graduate|  Male|   low|    Yes|        Urban|          Yes|\n",
      "|      LP001006|                 Y|             1|         0|Not Graduate|  Male|   low|    Yes|        Urban|           No|\n",
      "|      LP001008|                 Y|             1|         0|    Graduate|  Male|medium|     No|        Urban|           No|\n",
      "+--------------+------------------+--------------+----------+------------+------+------+-------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "api_url = \"https://raw.githubusercontent.com/platformps/LoanDataset/main/loan_data.json\"\n",
    "response = requests.get(api_url)\n",
    "loan_df = spark.read.json(spark.sparkContext.parallelize([response.json()]))\n",
    "loan_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e6572a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Application_ID: string (nullable = true)\n",
      " |-- Application_Status: string (nullable = true)\n",
      " |-- Credit_History: long (nullable = true)\n",
      " |-- Dependents: integer (nullable = true)\n",
      " |-- Education: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Income: string (nullable = true)\n",
      " |-- Married: string (nullable = true)\n",
      " |-- Property_Area: string (nullable = true)\n",
      " |-- Self_Employed: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_loan_df = loan_df.withColumn(\"Dependents\", f.regexp_replace(f.col(\"Dependents\"), \"\\+\", \"\")\\\n",
    "                                 .cast(IntegerType()))\n",
    "\n",
    "new_loan_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7878a2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_loan_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fa4247e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_inputs = [\"Application_Status\", \"Dependents\", \"Education\", \"Gender\", \"Income\", \"Married\", \"Property_Area\", \"Self_Employed\"]\n",
    "idx_outputs = [\"Application_Status_Index\", \"Dependents_Index\", \"Education_Index\", \"Gender_Index\", \"Income_Index\", \"Married_Index\", \"Property_Area_Index\", \"Self_Employed_Index\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e805433",
   "metadata": {},
   "source": [
    "Convert string column(s) into a numeric column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ec35cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column(s) from string to numerical values\n",
    "indexer = StringIndexer(inputCols=idx_inputs, outputCols=idx_outputs)\n",
    "indexed = indexer.fit(new_loan_df).transform(new_loan_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ad36ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "`np.string_` was removed in the NumPy 2.0 release. Use `np.bytes_` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[104], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m idx_encoded \u001b[38;5;241m=\u001b[39m \u001b[43mOneHotEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindex_encoded\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m cols_encoded \u001b[38;5;241m=\u001b[39m idx_encoded\u001b[38;5;241m.\u001b[39mfit(new_loan_df)\u001b[38;5;241m.\u001b[39mtransform(new_loan_df)\n",
      "File \u001b[1;32mc:\\Users\\james.byers\\Data_Engineer\\.data\\lib\\site-packages\\pyspark\\__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\james.byers\\Data_Engineer\\.data\\lib\\site-packages\\pyspark\\ml\\feature.py:3191\u001b[0m, in \u001b[0;36mOneHotEncoder.__init__\u001b[1;34m(self, inputCols, outputCols, handleInvalid, dropLast, inputCol, outputCol)\u001b[0m\n\u001b[0;32m   3189\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_java_obj(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.ml.feature.OneHotEncoder\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muid)\n\u001b[0;32m   3190\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs\n\u001b[1;32m-> 3191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetParams(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\james.byers\\Data_Engineer\\.data\\lib\\site-packages\\pyspark\\__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\james.byers\\Data_Engineer\\.data\\lib\\site-packages\\pyspark\\ml\\feature.py:3233\u001b[0m, in \u001b[0;36mOneHotEncoder.setParams\u001b[1;34m(self, inputCols, outputCols, handleInvalid, dropLast, inputCol, outputCol)\u001b[0m\n\u001b[0;32m   3227\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3228\u001b[0m \u001b[38;5;124;03msetParams(self, \\\\*, inputCols=None, outputCols=None, handleInvalid=\"error\", \\\u001b[39;00m\n\u001b[0;32m   3229\u001b[0m \u001b[38;5;124;03m          dropLast=True, inputCol=None, outputCol=None)\u001b[39;00m\n\u001b[0;32m   3230\u001b[0m \u001b[38;5;124;03mSets params for this OneHotEncoder.\u001b[39;00m\n\u001b[0;32m   3231\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3232\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs\n\u001b[1;32m-> 3233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\james.byers\\Data_Engineer\\.data\\lib\\site-packages\\pyspark\\ml\\param\\__init__.py:503\u001b[0m, in \u001b[0;36mParams._set\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 503\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtypeConverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid param value given for param \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (p\u001b[38;5;241m.\u001b[39mname, e))\n",
      "File \u001b[1;32mc:\\Users\\james.byers\\Data_Engineer\\.data\\lib\\site-packages\\pyspark\\ml\\param\\__init__.py:233\u001b[0m, in \u001b[0;36mTypeConverters.toString\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[1;32m--> 233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(value) \u001b[38;5;129;01min\u001b[39;00m [\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstring_\u001b[49m, np\u001b[38;5;241m.\u001b[39mstr_, np\u001b[38;5;241m.\u001b[39municode_]:\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(value)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\james.byers\\Data_Engineer\\.data\\lib\\site-packages\\numpy\\__init__.py:413\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr], name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __expired_attributes__:\n\u001b[1;32m--> 413\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    414\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` was removed in the NumPy 2.0 release. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m__expired_attributes__[attr]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    416\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    417\u001b[0m     )\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchararray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    420\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    421\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`np.chararray` is deprecated and will be removed from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe main namespace in the future. Use an array with a string \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor bytes dtype instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: `np.string_` was removed in the NumPy 2.0 release. Use `np.bytes_` instead."
     ]
    }
   ],
   "source": [
    "# idx_encoded = OneHotEncoder(inputCol=indexer, outputCol='index_encoded')\n",
    "# cols_encoded = idx_encoded.fit(new_loan_df).transform(new_loan_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7da35597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----+\n",
      "|Application_Status_Index|count|\n",
      "+------------------------+-----+\n",
      "|                     1.0|  164|\n",
      "|                     0.0|  347|\n",
      "+------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexed.groupBy(\"Application_Status_Index\").count().orderBy('count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "069f232f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|Application_Status|count|\n",
      "+------------------+-----+\n",
      "|                 N|  164|\n",
      "|                 Y|  347|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexed.groupBy(\"Application_Status\").count().orderBy('count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25abd9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|Income_Index|count|\n",
      "+------------+-----+\n",
      "|         0.0|  273|\n",
      "|         1.0|  193|\n",
      "|         2.0|   45|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dropped_index = indexer.drop(\"Application_ID\", \"Credit_History\", \"Application_Status\", \"Education\", \"Gender\", \"Income\", \"Married\", \"Property_Area\", \"Self_Employed\")\n",
    "# dropped_index.groupBy(dropped_index['Income_Index']).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598e7177",
   "metadata": {},
   "source": [
    "## Task 3 - Identify the label column and the input columns\n",
    "The VectorAssembler groups a bunch of inputCols as single column named \"features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f204e9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature vector\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"Income_Index\", \"Property_Area_Index\"], outputCol=\"features\")\n",
    "\n",
    "indexed_transformed = vectorAssembler.transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "013efabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------------+\n",
      "|         features|Application_Status_Index|\n",
      "+-----------------+------------------------+\n",
      "|[0.0,0.0,1.0,1.0]|                     0.0|\n",
      "|[2.0,0.0,1.0,2.0]|                     1.0|\n",
      "|    (4,[3],[1.0])|                     0.0|\n",
      "|[0.0,1.0,0.0,1.0]|                     0.0|\n",
      "|[0.0,0.0,1.0,1.0]|                     0.0|\n",
      "|[1.0,0.0,1.0,1.0]|                     0.0|\n",
      "|[0.0,1.0,0.0,1.0]|                     0.0|\n",
      "|    (4,[0],[3.0])|                     1.0|\n",
      "|[1.0,0.0,1.0,1.0]|                     0.0|\n",
      "|[2.0,0.0,2.0,0.0]|                     1.0|\n",
      "|[1.0,0.0,0.0,1.0]|                     0.0|\n",
      "|[1.0,0.0,0.0,1.0]|                     0.0|\n",
      "|    (4,[3],[2.0])|                     1.0|\n",
      "|[1.0,0.0,0.0,1.0]|                     0.0|\n",
      "|[0.0,0.0,1.0,1.0]|                     0.0|\n",
      "|    (4,[3],[1.0])|                     1.0|\n",
      "|[0.0,1.0,1.0,2.0]|                     1.0|\n",
      "|[0.0,1.0,1.0,1.0]|                     1.0|\n",
      "|[2.0,0.0,1.0,1.0]|                     0.0|\n",
      "|    (4,[1],[1.0])|                     1.0|\n",
      "+-----------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexed_transformed.select(\"features\", \"Application_Status_Index\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aa39db",
   "metadata": {},
   "source": [
    "## Taks 4 - Split the data\n",
    "Split the data set in the ratio of 70:30. 70% training data, 30% testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812593d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "(trainingData, testData) = indexed_transformed.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313773f6",
   "metadata": {},
   "source": [
    "## Task 5 - Build and Train a Logistic Regression Model\n",
    "Create a LR model and train the model using the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a805abe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 5 - create a linear regression instance\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"Application_Status_Index\")\n",
    "model = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931bb0e7",
   "metadata": {},
   "source": [
    "## Task 6 - Evaluate the model\n",
    "Your model is now trained. Use the testing data to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c6795427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on testing data\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f951a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "19fc9c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.6967741935483871\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='Application_Status_Index', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print('Accuracy = ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "34002b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisioin =  0.7126361888485366\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol='Application_Status_Index', predictionCol='prediction', metricName='weightedPrecision')\n",
    "precision = evaluator.evaluate(predictions)\n",
    "print('Precisioin = ', precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b9629e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall =  0.6967741935483871\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol='Application_Status_Index', predictionCol='prediction', metricName='weightedRecall')\n",
    "recall = evaluator.evaluate(predictions)\n",
    "print('Recall = ', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "813571bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score =  0.5945932632486364\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol='Application_Status_Index', predictionCol='prediction', metricName='f1')\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "print('F1 score = ', f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f109a5b0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881ecd4b",
   "metadata": {},
   "source": [
    "## Task 1 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd33581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67f61081",
   "metadata": {},
   "source": [
    "### Define pipeline stages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c1c183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9005486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 - scale the features using standard scaler\n",
    "# scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcc4629",
   "metadata": {},
   "source": [
    "## Task 3 - Build the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "44d39741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "# All the stages of the pipeline are mentioned in the order of execution.\n",
    "pipeline = Pipeline(stages=[vectorAssembler, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d459d852",
   "metadata": {},
   "source": [
    "## Task 4 - Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b656a6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "# (trainingData, testData) = dropped_index.randomSplit([0.8, .02], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1cd29a",
   "metadata": {},
   "source": [
    "## Task 5 - Fit the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51523b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline to the training data\n",
    "# ignore any warnings. The warnings are due to the simplified settings and the security settings of the lab\n",
    "\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2983d0",
   "metadata": {},
   "source": [
    "## Task 6 - Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "29eba2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "70fb6d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------------+---------------+------------+------------+-------------+-------------------+-------------------+-------------------+--------------------+--------------------+----------+\n",
      "|Dependents|Application_Status_Index|Education_Index|Gender_Index|Income_Index|Married_Index|Property_Area_Index|Self_Employed_Index|           features|       rawPrediction|         probability|prediction|\n",
      "+----------+------------------------+---------------+------------+------------+-------------+-------------------+-------------------+-------------------+--------------------+--------------------+----------+\n",
      "|         0|                     0.0|            0.0|         0.0|         0.0|          0.0|                0.0|                0.0|          (7,[],[])|[1.54754994474389...|[0.82455958599846...|       0.0|\n",
      "|         0|                     0.0|            0.0|         0.0|         0.0|          0.0|                0.0|                0.0|          (7,[],[])|[1.54754994474389...|[0.82455958599846...|       0.0|\n",
      "|         0|                     0.0|            0.0|         0.0|         0.0|          0.0|                1.0|                0.0|      (7,[5],[1.0])|[1.12816493692828...|[0.75550008573404...|       0.0|\n",
      "|         0|                     0.0|            0.0|         0.0|         0.0|          1.0|                0.0|                0.0|      (7,[4],[1.0])|[1.15516538766294...|[0.76045313099227...|       0.0|\n",
      "|         0|                     0.0|            0.0|         0.0|         1.0|          0.0|                1.0|                0.0|(7,[3,5],[1.0,1.0])|[0.92945106199660...|[0.71696390460337...|       0.0|\n",
      "+----------+------------------------+---------------+------------+------------+-------------+-------------------+-------------------+-------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc040058",
   "metadata": {},
   "source": [
    "Print the rmse value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f38988d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results = 0.5\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"Application_Status_Index\")\n",
    "ROC_AUC = evaluator.evaluate(predictions)\n",
    "print(\"Results =\", ROC_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a61ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96be593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605b1b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index = indexed.select(\"Dependents\", \"Education\", \"Education_Index\", \"Gender\", \"Gender_Index\", \"Income\", \"Income_Index\", \"Married\", \"Married_Index\", \"Property_Area\", \"Property_Area_Index\", \"Self_Employed\", \"Self_Employed_Index\", \"Application_Status\", \"Application_Status_Index\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3c1cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = spark.read.json(\"data\\cdw_sapp_customer.json\", multiLine=True)\n",
    "credit_df = spark.read.json(\"data\\cdw_sapp_credit.json\", multiLine=True)\n",
    "branch_df = spark.read.json(\"data\\cdw_sapp_branch.json\", multiLine=True)\n",
    "\n",
    "# all_df = spark.read.json([\"data\\cdw_sapp_branch.json\",\n",
    "                        # \"data\\cdw_sapp_credit.json\",\n",
    "                        # \"data\\cdw_sapp_customer.json\"\n",
    "                        # ], multiLine=True)\n",
    "# all_df.printSchema()\n",
    "# all_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b62b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_cust_joined = customer_df.join(credit_df, on='CREDIT_CARD_NO')\n",
    "credit_cust_joined.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44d1c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = customer_df.join(credit_df, on='CREDIT_CARD_NO').join(branch_df, on='BRANCH_CODE')\n",
    "joined.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c4a1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
